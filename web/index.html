<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Kyutai STT — Browser Speech-to-Text</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: 'SF Mono', Monaco, Consolas, monospace;
            max-width: 800px;
            margin: 0 auto;
            padding: 2rem;
            color: #111;
            background: #fafafa;
        }
        h1 {
            margin-bottom: 0.5rem;
            font-size: 2rem;
            font-weight: 600;
        }
        .subtitle {
            margin-bottom: 2rem;
            color: #666;
            font-size: 0.95rem;
        }
        .card {
            background: #fff;
            border: 1px solid #ddd;
            border-radius: 8px;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
        }
        .card h2 {
            font-size: 0.85rem;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            color: #888;
            margin-bottom: 1rem;
        }
        .status {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            margin-bottom: 1rem;
            font-size: 0.9rem;
        }
        .status-dot {
            width: 8px;
            height: 8px;
            border-radius: 50%;
            background: #999;
        }
        .status-dot.loading { background: #f0ad4e; }
        .status-dot.ready { background: #5cb85c; }
        .status-dot.error { background: #d9534f; }
        .status-dot.recording {
            background: #d9534f;
            animation: pulse 1s infinite;
        }
        @keyframes pulse {
            0%, 100% { opacity: 1; }
            50% { opacity: 0.5; }
        }
        .progress {
            height: 4px;
            background: #eee;
            border-radius: 2px;
            overflow: hidden;
            margin-bottom: 1rem;
            display: none;
        }
        .progress.visible {
            display: block;
        }
        .progress-bar {
            height: 100%;
            background: #007aff;
            width: 0%;
            transition: width 0.3s;
        }
        .controls {
            display: flex;
            gap: 0.5rem;
        }
        button {
            font-family: inherit;
            font-size: 0.95rem;
            padding: 0.6rem 1.2rem;
            cursor: pointer;
            border: 1px solid #ccc;
            background: #fff;
            border-radius: 6px;
            transition: all 0.15s;
        }
        button:hover:not(:disabled) {
            background: #f5f5f5;
            border-color: #999;
        }
        button:disabled {
            opacity: 0.4;
            cursor: not-allowed;
        }
        button.primary {
            background: #007aff;
            color: #fff;
            border-color: #007aff;
        }
        button.primary:hover:not(:disabled) {
            background: #0051d5;
            border-color: #0051d5;
        }
        button.recording {
            background: #d9534f;
            color: #fff;
            border-color: #d9534f;
            animation: pulse 1s infinite;
        }
        #transcript {
            margin-top: 0;
            padding: 1rem;
            min-height: 200px;
            border: 1px solid #eee;
            border-radius: 6px;
            background: #f9f9f9;
            white-space: pre-wrap;
            line-height: 1.6;
            font-size: 0.95rem;
            color: #111;
        }
        #transcript.empty {
            color: #999;
            font-style: italic;
        }
        button.test-file {
            font-size: 0.8rem;
            padding: 0.4rem 0.8rem;
            background: #f0f0f0;
        }
        .info {
            font-size: 0.85rem;
            color: #888;
            margin-top: 0.5rem;
        }
    </style>
</head>
<body>
    <h1>Kyutai STT 1B</h1>
    <p class="subtitle">Client-side speech-to-text (English + French) — runs entirely in your browser via WebGPU.</p>

    <div class="card">
        <h2>Status</h2>
        <div class="status">
            <div class="status-dot" id="statusDot"></div>
            <span id="statusText">Checking browser support...</span>
        </div>
        <div class="progress" id="progressBar">
            <div class="progress-bar" id="progressFill"></div>
        </div>
        <p class="info" id="progressInfo"></p>
    </div>

    <div class="card">
        <h2>Controls</h2>
        <div class="controls">
            <button id="recordBtn" class="primary" disabled>Start Recording</button>
            <button id="fileBtn" disabled>Load WAV File</button>
            <button id="resetBtn" disabled>Reset</button>
            <input type="file" id="fileInput" accept=".wav,audio/wav" style="display:none">
        </div>
        <div class="controls" style="margin-top:0.5rem">
            <button class="test-file" data-src="test-bria.wav" disabled>bria (EN, 30s)</button>
            <button class="test-file" data-src="test-crepes-fr.wav" disabled>crepes (FR, 30s)</button>
            <button class="test-file" data-src="test-loona.wav" disabled>loona (1s)</button>
        </div>
        <p class="info">Click Start Recording or load a test file. Audio stays on this device.</p>
    </div>

    <div class="card">
        <h2>Transcript</h2>
        <div id="transcript" class="empty">Transcript will appear here as you speak...</div>
    </div>

    <script type="module">
        const { SttClient } = await import('./stt-client.js?v=' + Date.now());

        // UI elements
        const statusDot = document.getElementById('statusDot');
        const statusText = document.getElementById('statusText');
        const progressBar = document.getElementById('progressBar');
        const progressFill = document.getElementById('progressFill');
        const progressInfo = document.getElementById('progressInfo');
        const transcriptEl = document.getElementById('transcript');
        const recordBtn = document.getElementById('recordBtn');
        const fileBtn = document.getElementById('fileBtn');
        const fileInput = document.getElementById('fileInput');
        const testFileBtns = document.querySelectorAll('.test-file');
        const resetBtn = document.getElementById('resetBtn');

        // State
        let client = null;
        let isRecording = false;
        let transcriptText = '';

        function updateStatus(state, text) {
            statusDot.className = 'status-dot ' + state;
            statusText.textContent = text;
        }

        function formatBytes(bytes) {
            const mb = bytes / (1024 * 1024);
            return mb >= 1 ? `${mb.toFixed(1)} MB` : `${(bytes / 1024).toFixed(0)} KB`;
        }

        // Check WebGPU support
        if (!navigator.gpu) {
            updateStatus('error', 'WebGPU not supported. Use Chrome or Edge.');
            statusText.textContent = 'WebGPU not supported in this browser. Please use Chrome or Edge (version 113+).';
        } else {
            // Initialize client
            // For embedding on another domain, configure baseUrl and optional overrides:
            //   baseUrl: 'https://cdn.example.com/stt',
            //   shardList: ['shard-00000.gguf', 'shard-00001.gguf', ...],
            //   mimiUrl: 'https://cdn.example.com/stt/models/mimi.safetensors',
            //   tokenizerUrl: 'https://cdn.example.com/stt/models/tokenizer.model',
            client = new SttClient({
                onStatus: (text, ready, progress) => {
                    if (ready) {
                        updateStatus('ready', text);
                        recordBtn.disabled = false;
                        fileBtn.disabled = false;
                        testFileBtns.forEach(b => b.disabled = false);
                        resetBtn.disabled = false;
                        progressBar.classList.remove('visible');
                        progressInfo.textContent = '';
                    } else {
                        updateStatus('loading', text);

                        if (progress) {
                            progressBar.classList.add('visible');
                            const percent = progress.total > 0
                                ? (progress.loaded / progress.total) * 100
                                : 0;
                            progressFill.style.width = percent + '%';
                            progressInfo.textContent = `${formatBytes(progress.loaded)} / ${formatBytes(progress.total)}`;
                        }
                    }
                },

                onTranscript: (text, isFinal) => {
                    if (text) {
                        if (isFinal) {
                            transcriptText += text + ' ';
                        } else {
                            transcriptText += text;
                        }
                        transcriptEl.textContent = transcriptText;
                        transcriptEl.classList.remove('empty');
                    }
                },

                onError: (err) => {
                    updateStatus('error', 'Error: ' + err.message);
                    console.error(err);
                }
            });

            // Initialize and load model
            (async () => {
                try {
                    await client.init();
                } catch (err) {
                    updateStatus('error', 'Failed to initialize: ' + err.message);
                    console.error(err);
                }
            })();
        }

        // Record button
        recordBtn.addEventListener('click', async () => {
            if (!isRecording) {
                // Start recording
                try {
                    await client.startRecording();
                    isRecording = true;
                    recordBtn.textContent = 'Stop Recording';
                    recordBtn.classList.add('recording');
                    recordBtn.classList.remove('primary');
                    statusDot.classList.add('recording');
                    updateStatus('recording', 'Recording...');
                } catch (err) {
                    updateStatus('error', 'Microphone error: ' + err.message);
                    console.error(err);
                }
            } else {
                // Stop recording
                client.stopRecording();
                isRecording = false;
                recordBtn.textContent = 'Start Recording';
                recordBtn.classList.remove('recording');
                recordBtn.classList.add('primary');
                statusDot.classList.remove('recording');
                updateStatus('ready', 'Ready');
            }
        });

        // Shared: decode audio and feed to worker
        async function feedAudioFile(arrayBuf, label) {
            updateStatus('loading', `Decoding ${label}...`);
            fileBtn.disabled = true;
            recordBtn.disabled = true;
            testFileBtns.forEach(b => b.disabled = true);

            try {
                // Decode to 24kHz mono (Mimi's native rate)
                const offlineCtx = new OfflineAudioContext(1, 1, 24000);
                const audioBuf = await offlineCtx.decodeAudioData(arrayBuf);

                // Resample to 24kHz mono
                const duration = audioBuf.duration;
                const outLen = Math.ceil(duration * 24000);
                const resampleCtx = new OfflineAudioContext(1, outLen, 24000);
                const source = resampleCtx.createBufferSource();
                source.buffer = audioBuf;
                source.connect(resampleCtx.destination);
                source.start(0);
                const rendered = await resampleCtx.startRendering();
                const pcm = rendered.getChannelData(0);

                console.log(`[file] Decoded: ${pcm.length} samples (${(pcm.length/24000).toFixed(1)}s) at 24kHz`);
                updateStatus('loading', `Feeding ${(pcm.length/24000).toFixed(1)}s of audio...`);

                // Reset state before feeding
                client.worker.postMessage({ type: 'reset' });
                transcriptText = '';
                transcriptEl.textContent = '';
                transcriptEl.classList.remove('empty');

                // Feed in ~100ms chunks (2400 samples)
                const CHUNK = 2400;
                for (let i = 0; i < pcm.length; i += CHUNK) {
                    const chunk = pcm.slice(i, Math.min(i + CHUNK, pcm.length));
                    client.worker.postMessage(
                        { type: 'audio', samples: chunk },
                        [chunk.buffer]
                    );
                    // Yield to let worker process and UI update
                    if (i % (CHUNK * 10) === 0) {
                        await new Promise(r => setTimeout(r, 0));
                    }
                }

                // Flush remaining tokens
                client.worker.postMessage({ type: 'stop' });
                updateStatus('ready', `Done — fed ${(pcm.length/24000).toFixed(1)}s from ${label}`);
            } catch (err) {
                updateStatus('error', 'File error: ' + err.message);
                console.error(err);
            } finally {
                fileBtn.disabled = false;
                recordBtn.disabled = false;
                testFileBtns.forEach(b => b.disabled = false);
            }
        }

        // File picker button
        fileBtn.addEventListener('click', () => fileInput.click());
        fileInput.addEventListener('change', async (e) => {
            const file = e.target.files[0];
            if (!file) return;
            fileInput.value = '';
            await feedAudioFile(await file.arrayBuffer(), file.name);
        });

        // Test file buttons — fetch from server and feed
        testFileBtns.forEach(btn => {
            btn.addEventListener('click', async () => {
                const src = btn.dataset.src;
                const resp = await fetch(src);
                if (!resp.ok) {
                    updateStatus('error', `Failed to fetch ${src}`);
                    return;
                }
                await feedAudioFile(await resp.arrayBuffer(), src);
            });
        });

        // Reset button
        resetBtn.addEventListener('click', () => {
            transcriptText = '';
            transcriptEl.textContent = 'Transcript will appear here as you speak...';
            transcriptEl.classList.add('empty');
            client.reset();
            updateStatus('ready', 'Ready');
        });
    </script>
</body>
</html>

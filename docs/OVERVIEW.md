# Kyutai STT 1B â†’ Browser (WASM + WebGPU)

## What we're building

A browser-native, fully client-side speech-to-text engine based on Kyutai's `stt-1b-en_fr` model. No server, no API keys, no data leaves the device. The user opens a web page, the model downloads once (~500MB, cached), and from then on they have real-time streaming transcription from their microphone â€” entirely in their browser tab.

## Why this model

Kyutai STT 1B is the sweet spot:

| Model | Params | Browser-viable? | Streaming? | Quality |
|---|---|---|---|---|
| Whisper base | 73M | âœ… trivial (~200MB) | âŒ chunked | decent |
| Moonshine Base | ~400M | âœ… easy (~150MB) | âœ… | good |
| **Kyutai STT 1B** | **1B** | **âœ… feasible (~500MB Q4)** | **âœ… native streaming** | **excellent** |
| Voxtral Realtime 4B | 4B | ğŸŸ¡ tight (~2.5GB Q4) | âœ… | SOTA |

Kyutai STT 1B uses "delayed streams modeling" â€” audio and text are modeled as parallel time-aligned streams. This means the model is *natively* streaming: it starts outputting text ~500ms after speech begins, with no chunking hacks. It includes a semantic VAD (voice activity detection) that predicts when the user has stopped talking. English + French. CC-BY 4.0 license.

## Architecture

```
Microphone (browser)
  â†’ AudioWorklet (16kHz mono PCM)
    â†’ Web Worker
      â†’ Mimi codec [WASM, CPU] (audio samples â†’ 12.5Hz token frames)
        â†’ STT Transformer [WASM+WebGPU] (audio tokens â†’ text tokens)
          â†’ Detokenizer â†’ text string
            â†’ postMessage to main thread â†’ UI
```

### Component breakdown

**1. Mimi audio codec (~25M params)**
- Converts raw audio into discrete tokens at 12.5Hz (32 codebook tokens per frame)
- Already implemented in pure Rust (`rustymimi` in `kyutai-labs/moshi`)
- Small enough to run on CPU via WASM. No GPU needed for this part

**2. STT Transformer (~1B params)**
- Decoder-only transformer consuming the Mimi tokens
- Predicts text tokens on a parallel stream, offset by ~6 frames (500ms) from audio
- This is the expensive part â€” must run on WebGPU
- Needs: embeddings, multi-head attention with RoPE, KV cache, linear head

**3. Delayed streams logic**
- The model doesn't do "listen then transcribe" (encoder-decoder like Whisper)
- Instead, audio and text streams run in parallel, with text delayed by a fixed offset
- At each step: consume the next audio frame, predict the text token for N frames ago
- Padding tokens fill the text stream until real text appears
- The semantic VAD predicts end-of-speech probability at each frame

## Key reference implementations

### `kyutai-labs/delayed-streams-modeling` + `kyutai-labs/moshi`
- The official implementation. PyTorch (research), Rust/Candle (production), MLX (Apple)
- The Rust/Candle code is the closest to what we need, but Candle has no WebGPU backend
- `rustymimi` is reusable as-is for the Mimi codec
- Weight format: safetensors (bf16), Candle q8

### `TrevorS/voxtral-mini-realtime-rs`
- Solved the exact same class of problem: STT transformer â†’ Rust â†’ WASM + WebGPU
- Uses the Burn framework with its `wgpu` backend (works native + browser)
- Q4 GGUF quantization with custom WGSL dequant shaders
- Solved all the hard WASM constraints: 2GB allocation limit, async GPU readback, workgroup size limits
- **This is our architectural template.** Many patterns copy directly

## Tech stack

| Layer | Choice | Why |
|---|---|---|
| ML framework | **Burn** (`burn-rs`) | Has native `wgpu` backend â†’ runs on WebGPU in browser. Candle doesn't |
| GPU compute | **wgpu** (via Burn) | Cross-platform: Vulkan/Metal/DX12 native, WebGPU in browser. Same code path |
| WASM toolchain | **wasm-pack** + `wasm-bindgen` | Standard Rustâ†’WASM pipeline. Generates JS bindings automatically |
| Quantization | **Q4** (GGUF or custom) | 1B params Ã— 4 bits Ã· 8 = ~500MB. Fits in browser memory comfortably |
| Audio codec | **rustymimi** (existing Rust) | Already done. Compile to WASM, runs on CPU |
| Audio capture | **AudioWorklet API** | Low-latency mic input in the browser. Feeds PCM chunks to the worker |
| Threading | **Web Worker** | All inference runs off the main thread. UI stays responsive |
| Model hosting | **HuggingFace Hub** | Free CDN, CORS-friendly, standard for ML models. Cached via Cache API |

## Quantization math

```
1B params at Q4:
  1,000,000,000 Ã— 4 bits = 4,000,000,000 bits = 500,000,000 bytes â‰ˆ 500 MB

WASM constraints:
  - Single ArrayBuffer max: 2GB (4GB with memory64, but not universal) âœ… fine
  - Total memory budget: ~4GB practical                                âœ… fine
  - WebGPU buffer max: implementation-dependent, usually 1-2GB         âœ… fine

For comparison:
  - Voxtral 4B Q4 = ~2.5GB (proven to work in browser)
  - Moonshine Base = ~150MB
  - Our target 500MB is comfortably in the middle
```

## Risks and mitigations

| Risk | Severity | Mitigation |
|---|---|---|
| Q4 quantization hurts 1B model accuracy too much | Medium | Eval WER early. Fall back to Q8 (~1GB) â€” still browser-viable |
| Burn's wgpu backend missing ops needed by the model | Medium | voxtral-rs already proved the path for attention + RoPE + KV cache. Copy their custom WGSL shaders. File issues upstream for anything else |
| Mimi codec too slow in WASM-CPU | Low | Mimi is only ~25M params, runs at 12.5Hz. Even WASM-CPU should handle real-time. If not, port Mimi to Burn+wgpu too |
| Delayed-streams logic is subtle to port correctly | Medium | The PyTorch reference is clean. Port line-by-line, verify frame-by-frame against reference outputs on test audio |
| WebGPU workgroup size limits (256 invocations) | Low | voxtral-rs already patched `cubecl-wgpu` for this. Reuse their patch |
| Browser memory pressure on lower-end devices | Low | 500MB model + ~200MB working memory. Any machine with 4GB+ RAM is fine. Show a "your device may not support this" warning if needed |

## Deliverables

The project produces two things:

### 1. Model artifact
- `stt-1b-en_fr-q4.gguf` (or shards) uploaded to HuggingFace
- Verified: WER on LibriSpeech test-clean within acceptable delta of original

### 2. Code repository
```
kyutai-stt-browser/
â”œâ”€â”€ CLAUDE.md                  # project context for Claude Code agents
â”œâ”€â”€ Cargo.toml                 # workspace: mimi-wasm + stt-wasm crates
â”œâ”€â”€ crates/
â”‚   â”œâ”€â”€ mimi-wasm/             # Mimi codec compiled to WASM
â”‚   â”‚   â”œâ”€â”€ src/lib.rs         # stripped-down rustymimi, wasm-bindgen exports
â”‚   â”‚   â””â”€â”€ Cargo.toml
â”‚   â””â”€â”€ stt-wasm/              # STT transformer in Burn+wgpu
â”‚       â”œâ”€â”€ src/
â”‚       â”‚   â”œâ”€â”€ lib.rs         # wasm-bindgen entry points
â”‚       â”‚   â”œâ”€â”€ model.rs       # transformer: embeddings, attention, RoPE, KV cache
â”‚       â”‚   â”œâ”€â”€ gguf.rs        # Q4 weight loader
â”‚       â”‚   â”œâ”€â”€ wgsl/          # custom compute shaders (Q4 dequant + matmul)
â”‚       â”‚   â””â”€â”€ stream.rs      # delayed-streams decoding logic
â”‚       â””â”€â”€ Cargo.toml
â”œâ”€â”€ web/
â”‚   â”œâ”€â”€ index.html             # standalone demo page
â”‚   â”œâ”€â”€ worker.js              # web worker: loads WASM, orchestrates pipeline
â”‚   â”œâ”€â”€ audio-processor.js     # AudioWorklet: mic â†’ PCM chunks
â”‚   â””â”€â”€ stt-client.js          # optional: clean JS API for embedding
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ quantize.py            # convert safetensors â†’ Q4 GGUF
â”‚   â”œâ”€â”€ eval.py                # WER evaluation script
â”‚   â””â”€â”€ gen-cert.sh            # self-signed cert for local HTTPS dev
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ reference/             # test audio files + expected transcripts
â”‚   â”œâ”€â”€ test_mimi.rs           # mimi codec correctness tests
â”‚   â”œâ”€â”€ test_model.rs          # model inference tests (native)
â”‚   â””â”€â”€ e2e_browser.spec.ts    # Playwright end-to-end browser test
â””â”€â”€ README.md
```

The `web/` directory is self-contained. Copy it to any static file server and it works.
